{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/cwq7nkr55jx38bdscyq3lnl00000gp/T/ipykernel_81547/3458263502.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('NolEngine_Claude.pt', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('NolEngine_Claude.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm.notebook import trange\n",
    "import math\n",
    "\n",
    "# Enhanced hyperparameters\n",
    "batch_size = 32  # increased for better gradient estimates\n",
    "block_size = 128  # increased context length\n",
    "max_iters = 20000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4  # slightly lower for better stability\n",
    "warmup_iters = 1000  # learning rate warmup\n",
    "min_lr = 1e-5  # learning rate floor\n",
    "weight_decay = 0.1  # L2 regularization\n",
    "grad_clip = 1.0  # gradient clipping\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128  # increased embedding dimension\n",
    "n_head = 8    # increased number of heads\n",
    "n_layer = 6   # increased number of layers\n",
    "dropout = 0.1 # added dropout for regularization\n",
    "vocab_size = 1025\n",
    "\n",
    "import pickle\n",
    "with open ('tokened_text', 'rb') as fp:\n",
    "    tokened_text = pickle.load(fp)\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(tokened_text, dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias \"\"\"\n",
    "    def __init__(self, ndim, bias=False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head self-attention with improved efficiency \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        # causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                    .view(1, 1, block_size, block_size))\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Enhanced feedforward network with GELU activation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd, bias=False)\n",
    "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gelu    = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block with improved architecture \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(n_embd, bias=False)\n",
    "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
    "        self.ln_2 = LayerNorm(n_embd, bias=False)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ffwd(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class ImprovedLanguageModel(nn.Module):\n",
    "    \"\"\" Enhanced language model with improved architecture and training stability \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = LayerNorm(n_embd, bias=False)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Enhanced generation with temperature and top-k sampling\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "def get_lr(it):\n",
    "    # Learning rate schedule: linear warmup and cosine decay\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bx/cwq7nkr55jx38bdscyq3lnl00000gp/T/ipykernel_81547/1360019767.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('NolEngine_Claude.pt', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImprovedLanguageModel().to(device)  # Initialize the model architecture\n",
    "model.load_state_dict(torch.load('NolEngine_Claude.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BPETokenizer\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load(\"NolEngine.model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batman’s breath...\n",
      "DUCARD (CONT’D): (superned)\n",
      "Oppie? Which is?\n",
      "THUG: Nothing. But he went to us like\n",
      "it.\n",
      "Ducard TO BLACKS the black of his face - mouthes to\n",
      "spot Rachel’s going down.\n",
      "(CONTINUED):\n",
      "CONTINUED: (2): 59.\n",
      "WAYNE: What about the restaurant?\n",
      "DUCARD: That’s your truth, I know what\n",
      "you hearful minutes. And even if you\n",
      "can trust might not understand. And you\n",
      "thought the price of the\n",
      "city. Serig calculates instructs\n",
      "will be condirection. Robert.\n",
      "ALFRED: Memory good of those calculations,\n",
      "and we can’t strum asking enough\n",
      "abour passential lovely.\n",
      "Wayne turns to Alfred, who is switeing. Take up on\n",
      "the sound. Ducard leads Al Ghul party. Even, breathing away-\n",
      "Ducard and his foot flow puse.\n",
      "RA’S AL GHUL: I have made it until I was warry.\n",
      "Wayne takes out a dry innerner and lets it return off.\n",
      "\n",
      "INT. CLASS ABIN, WAYNE ENTERPRISES -- CONTINUOUS\n",
      "\n",
      "Wayne’s eyes are closer to restaurant. Turns to Alfred.\n",
      "RACHEL: Your new instruct. And the whole motions of\n",
      "power of scum of psyl-\n",
      "RACHEL: These journality.\n",
      "Wayne looks at Alfreez. Makes the pain drinks.\n",
      "(CONTINUED):\n",
      "CONTINUED::\n",
      "SHBARE S THUG: What?!\n",
      "RACHEL: I need to know you, Frank you.\n",
      "(CONTINUED):\n",
      "CONTINUED: (2): 36.\n",
      "WAYNE: (shrugs)\n",
      "Something you’re talking about he\n",
      "couldn’t run a raky.\n",
      "Wayne shakes his head. Earle considers him his face, fraufaced...\n",
      "WAYNE: Any\n",
      "gattoo.\n",
      "GORDON: Don’t make this boat in we’s\n",
      "wearing where, but the flat stuff.\n",
      "DUCARD (CONT’D): Fear enough to press, expect\n",
      "any more vocue.\n",
      "EARLE: (over radio)\n",
      "I’ve got pretty faniously\n",
      "interested in a mask, beyond left to\n",
      "pass.\n",
      "WAYNE: (grins)\n",
      "Not showing demonstrative.'\n",
      "Earl interesting clown mine.\n",
      "EARLE: What have you stinned intact?\n",
      "Earle looks up at the edge of the board, fatter.\n",
      "BARBARA: You’re a chance again, and it\n",
      "that name isn’t.\n",
      "Earl watches a small slips it up onto the bar, in\n",
      "louds talk.\n",
      "Earle is uneasy. Earl is a Chald CARD in coffee with an arrow and\n",
      "almost Guard’s hands, and we:\n",
      "CUT TO::\n",
      "\n",
      "INT. ROOM 2022, ATOMIC ENERGY COMMISSION -- DAY (COLOUR)\n",
      "\n",
      "I look up to the window. Tom is drunkable to leave.\n",
      "TOM: To say I control it was\n",
      "to believe the interest\n",
      "position.\n",
      "I see that believed your stuff.\n",
      "I turn to look at my home-\n",
      "TOM: Not it. I don’t know anything. I\n",
      "just following you a good leave me\n",
      "to kill you that means we wis, right.\n",
      "Cooper sits at a dust as she’s\n",
      "gone.\n",
      "Teddy stris a covered window, emb\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[616, 598]], dtype=torch.long, device=device)\n",
    "print(tokenizer.decode(model.generate(context, max_new_tokens=1000, temperature=0.7, top_k=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ImprovedLanguageModel().to(device)\n",
    "\n",
    "\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "for iter in trange(max_iters):\n",
    "    # Learning rate schedule\n",
    "    lr = get_lr(iter)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
